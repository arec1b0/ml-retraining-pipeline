---
alwaysApply: true
---
# Python MLOps and AI Engineering Rules

## Framework and Library Standards

### Core MLOps Stack
- Use **MLflow** for experiment tracking, model registry, and lifecycle management
- Implement **DVC** for data version control and pipeline management
- Use **Hydra** or **dynaconf** for configuration management
- Implement **Prefect** or **Apache Airflow** for workflow orchestration
- Use **FastAPI** for model serving and API development
- Implement **Docker** for containerization and environment consistency

### Monitoring and Observability
- Use **Evidently AI** or **WhyLabs** for data drift detection
- Implement **Prometheus + Grafana** for metrics and monitoring
- Use **structlog** for structured logging in JSON format
- Implement **OpenTelemetry** for distributed tracing

## Code Quality and Structure

### Python Best Practices
- Use **Python 3.9+** with type hints throughout the codebase
- Follow **PEP 8** style guidelines with **black** formatter
- Use **ruff** for fast Python linting and code analysis
- Implement **pre-commit** hooks for automated code quality checks
- Use **poetry** or **pip-tools** for dependency management

### Project Structure Standards
```
project_root/
├── configs/              # Configuration files (YAML/TOML)
├── data/
│   ├── raw/             # Original, immutable data
│   ├── processed/       # Cleaned, feature-engineered data
│   └── external/        # Third-party data sources
├── src/
│   ├── data/            # Data ingestion and processing
│   ├── features/        # Feature engineering
│   ├── models/          # Model training and inference
│   ├── pipelines/       # ML pipelines and workflows
│   └── utils/           # Utility functions
├── tests/               # Unit and integration tests
├── notebooks/           # Jupyter notebooks for exploration
├── artifacts/           # Model artifacts and outputs
├── docs/               # Project documentation
└── .cursor/rules/      # Cursor AI rules
```

### Error Handling and Logging
- Implement comprehensive error handling with custom exceptions
- Use structured logging with correlation IDs for request tracking
- Log model predictions, performance metrics, and data quality issues
- Implement circuit breakers for external service calls

## Version Control and Reproducibility

### Git and DVC Integration
- Version control code with **Git** and data/models with **DVC**
- Use semantic versioning for model releases (MAJOR.MINOR.PATCH)
- Tag experiments with meaningful names and metadata
- Implement **Git hooks** for automated testing and validation

### Environment Management
- Use **Docker** for consistent development and production environments
- Define clear **requirements.txt** or **pyproject.toml** with pinned versions
- Document Python version requirements and system dependencies
- Use **.env** files for environment-specific configurations

## Configuration Management

### Configuration Patterns
- Separate configuration from code using YAML/TOML files
- Implement environment-specific configs (dev, staging, prod)
- Use configuration schemas for validation
- Support configuration inheritance and overrides

### Example Configuration Structure
```yaml
# configs/training.yaml
job:
  name: "model_training"
  version: "v1.0.0"

data:
  input_path: "data/processed/train.parquet"
  validation_split: 0.2

model:
  name: "RandomForestClassifier"
  parameters:
    n_estimators: 100
    max_depth: 10
    random_state: 42

mlflow:
  experiment_name: "customer_churn_prediction"
  tracking_uri: "sqlite:///mlruns.db"
```

## Testing Standards

### Test Coverage Requirements
- Maintain **>90%** code coverage for core ML modules
- Implement **unit tests** for all data processing functions
- Create **integration tests** for ML pipelines
- Use **pytest** with fixtures for test data management
- Implement **data validation tests** with **Great Expectations** or **Pandera**

### Model Testing Patterns
- Test model training reproducibility
- Validate model performance on holdout datasets
- Implement **A/B testing** frameworks for model comparison
- Test model inference latency and throughput
- Validate model outputs against expected ranges

## Deprecated Patterns to Avoid

### Legacy MLOps Practices
- ❌ Don't use Jupyter notebooks for production code
- ❌ Avoid hardcoded file paths and configuration values
- ❌ Don't train models without experiment tracking
- ❌ Avoid deploying models without proper versioning
- ❌ Don't skip data validation and quality checks
- ❌ Avoid manual model deployment processes

### Anti-Patterns
- ❌ Don't mix data science exploration with production code
- ❌ Avoid large monolithic scripts without modular functions
- ❌ Don't ignore model drift and performance degradation
- ❌ Avoid storing credentials in code or config files
- ❌ Don't skip logging for model predictions and errors