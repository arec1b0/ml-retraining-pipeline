---
alwaysApply: true
---
# Data Engineering and ML Pipeline Rules

## Data Pipeline Architecture

### Pipeline Design Patterns
- Use **Directed Acyclic Graph (DAG)** architecture for pipeline dependencies
- Implement **idempotent operations** that can be safely retried
- Design **stateless transformations** for better scalability
- Use **checkpoint mechanisms** for long-running pipeline recovery
- Implement **backpressure handling** for streaming data processing

### Data Processing Standards
- Use **Apache Spark** or **Dask** for distributed data processing
- Implement **pandas** with **pyarrow** backend for efficient data operations
- Use **Polars** for high-performance DataFrame operations
- Apply **lazy evaluation** patterns where possible
- Implement **schema validation** at data ingestion points

## Data Ingestion Patterns

### Batch Processing
```python
from prefect import task, flow
from src.data.loaders import DataLoader
from src.data.validators import DataValidator

@task
def ingest_batch_data(source_config: dict) -> pd.DataFrame:
    """Ingest data with validation and error handling."""
    loader = DataLoader(source_config)
    raw_data = loader.extract()
    
    validator = DataValidator()
    validated_data = validator.validate_schema(raw_data)
    
    return validated_data

@flow
def batch_ingestion_pipeline():
    """Orchestrate batch data ingestion."""
    return ingest_batch_data({"source": "s3://bucket/data"})
```

### Streaming Processing
- Use **Apache Kafka** or **Pulsar** for message streaming
- Implement **exactly-once processing** semantics
- Handle **late-arriving data** with windowing strategies
- Use **Faust** or **Kafka-python** for stream processing
- Implement **dead letter queues** for failed messages

## Feature Engineering Standards

### Feature Store Integration
- Use **Feast** or **Tecton** for feature store management
- Implement **point-in-time correct** feature joins
- Create **reusable feature transformations**
- Maintain **feature lineage** and documentation
- Implement **feature monitoring** for drift detection

### Transformation Patterns
```python
from dataclasses import dataclass
from typing import Protocol
import pandas as pd

class FeatureTransformer(Protocol):
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """Transform raw data into features."""
        ...

@dataclass
class NumericFeatureTransformer:
    """Standard numeric feature transformations."""
    
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        # Apply standardization, normalization, binning
        return df.pipe(self._standardize).pipe(self._handle_outliers)
    
    def _standardize(self, df: pd.DataFrame) -> pd.DataFrame:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])
        return df
```

## Data Quality and Validation

### Schema Validation
- Use **Pandera** or **Great Expectations** for data validation
- Implement **schema evolution** handling
- Validate **data types, ranges, and constraints**
- Create **data quality reports** and alerts
- Implement **anomaly detection** for data quality monitoring

### Example Validation Patterns
```python
import pandera as pa
from pandera import Column, DataFrameSchema

# Define schema for incoming data
user_events_schema = DataFrameSchema({
    "user_id": Column(str, checks=pa.Check.str_length(min_val=1)),
    "event_timestamp": Column("datetime64[ns]"),
    "event_type": Column(str, checks=pa.Check.isin(["click", "view", "purchase"])),
    "revenue": Column(float, checks=pa.Check.ge(0), nullable=True)
})

@pa.check_types
def validate_user_events(df: pa.DataFrame[user_events_schema]) -> pd.DataFrame:
    """Validate user events data with Pandera schema."""
    return df
```

## Pipeline Orchestration

### Workflow Management
- Use **Prefect 2.0** for modern workflow orchestration
- Implement **parameterized flows** for reusability
- Use **task caching** for expensive operations
- Implement **conditional logic** and **dynamic workflows**
- Create **monitoring dashboards** for pipeline health

### Error Handling and Retries
```python
from prefect import task, flow
from prefect.tasks import exponential_backoff
from typing import Optional

@task(retries=3, retry_delay_seconds=exponential_backoff(backoff_factor=2))
def robust_data_processing(data_path: str) -> Optional[pd.DataFrame]:
    """Process data with automatic retries and error handling."""
    try:
        return pd.read_parquet(data_path)
    except Exception as e:
        logger.error(f"Failed to process {data_path}: {e}")
        raise
```

## Data Storage Patterns

### Data Lake Architecture
- Implement **medallion architecture** (bronze, silver, gold layers)
- Use **Delta Lake** or **Apache Iceberg** for ACID transactions
- Implement **partitioning strategies** for query performance
- Use **columnar formats** (Parquet, ORC) for analytics
- Implement **data retention policies** and **lifecycle management**

### Database Integration
- Use **SQLAlchemy** with **alembic** for schema migrations
- Implement **connection pooling** for database efficiency
- Use **read replicas** for analytical queries
- Implement **database monitoring** and **query optimization**
- Use **async database drivers** for high-throughput applications

## Monitoring and Observability

### Pipeline Monitoring
- Track **data volume, velocity, and variety** metrics
- Monitor **pipeline execution time** and **resource usage**
- Implement **data lineage tracking**
- Create **SLA monitoring** for critical data pipelines
- Use **custom metrics** for business KPIs

### Alerting Strategies
```python
from prefect.blocks.notifications import SlackWebhook
from prefect import get_run_logger

async def send_pipeline_alert(message: str, severity: str = "INFO"):
    """Send pipeline alerts to monitoring systems."""
    logger = get_run_logger()
    logger.info(f"Pipeline alert: {message}")
    
    if severity in ["ERROR", "CRITICAL"]:
        slack_webhook = await SlackWebhook.load("mlops-alerts")
        await slack_webhook.notify(message)
```

## Performance Optimization

### Memory Management
- Use **chunked processing** for large datasets
- Implement **memory-efficient data types** (category, sparse)
- Use **garbage collection** strategies
- Monitor **memory usage** and **optimize DataFrame operations**
- Implement **data compression** for storage efficiency

### Compute Optimization
- Use **vectorized operations** instead of loops
- Implement **parallel processing** with **multiprocessing** or **joblib**
- Use **GPU acceleration** with **CuPy** or **Rapids** when appropriate
- Profile code with **cProfile** and **memory_profiler**
- Implement **caching strategies** for repeated computations

## Anti-Patterns to Avoid

### Data Pipeline Anti-Patterns
- ❌ Don't create **tightly coupled pipeline steps**
- ❌ Avoid **processing all data in memory** for large datasets
- ❌ Don't ignore **data quality validation**
- ❌ Avoid **hardcoded assumptions** about data structure
- ❌ Don't skip **error handling** and **retry mechanisms**
- ❌ Avoid **synchronous processing** for independent operations
- ❌ Don't ignore **pipeline monitoring** and **alerting**