{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration & Validation Setup\n",
    "\n",
    "**Objective:** Load the raw and reference datasets to understand their structure, distribution, and content. This notebook is also used to scaffold our initial Great Expectations (GE) suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "from great_expectations.cli.datasource import GxDatasourceWarning\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress datasource warnings from GE\n",
    "warnings.filterwarnings(\"ignore\", category=GxDatasourceWarning)\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "We'll load both the `raw` data (which simulates incoming data) and the `reference` data (our \"golden\" set for drift comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths (assuming notebook is run from project root)\n",
    "RAW_DATA_PATH = \"../data/raw/feedback.csv\"\n",
    "REFERENCE_DATA_PATH = \"../data/reference/sentiment_reference.csv\"\n",
    "\n",
    "# Check if paths are correct, adjust if running from inside /notebooks dir\n",
    "if not os.path.exists(RAW_DATA_PATH):\n",
    "    RAW_DATA_PATH = \"data/raw/feedback.csv\"\n",
    "    REFERENCE_DATA_PATH = \"data/reference/sentiment_reference.csv\"\n",
    "    # Change CWD to project root\n",
    "    if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "        os.chdir(\"..\")\n",
    "        print(f\"Changed directory to: {os.getcwd()}\")\n",
    "\n",
    "raw_df = pd.read_csv(RAW_DATA_PATH)\n",
    "ref_df = pd.read_csv(REFERENCE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data (`feedback.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw Data Shape: {raw_df.shape}\")\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Data (`sentiment_reference.csv`)\n",
    "\n",
    "This dataset includes a `prediction` column, simulating the output of a model that was run on this data. This is crucial for *model performance monitoring* with Evidently AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reference Data Shape: {ref_df.shape}\")\n",
    "ref_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Value Distributions\n",
    "\n",
    "Let's look at the distribution of the target variable, `sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Raw Data Sentiment Distribution ---\")\n",
    "print(raw_df['sentiment'].value_counts(normalize=True))\n",
    "print(\"\\n--- Reference Data Sentiment Distribution ---\")\n",
    "print(ref_df['sentiment'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Great Expectations\n",
    "\n",
    "We can use this notebook to create our first \"Expectation Suite\". We'll base our initial suite on the `reference_data.csv` file, as it represents our \"golden standard\" for data.\n",
    "\n",
    "**Note:** You must run `great_expectations init` in your terminal *before* running the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the GE Data Context\n",
    "context = ge.get_context()\n",
    "print(\"Great Expectations context loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Datasource\n",
    "\n",
    "First, we tell GE where our data lives. We'll set up a Pandas datasource pointing to the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datasource = context.sources.add_pandas(\"pandas_data_source\")\n",
    "    print(\"Datasource 'pandas_data_source' added.\")\n",
    "except Exception as e:\n",
    "    print(f\"Datasource already exists or error: {e}\")\n",
    "    datasource = context.datasources[\"pandas_data_source\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Assets\n",
    "\n",
    "Now, we define specific \"assets\" within that datasource. We'll create one for our `reference` data, which we will use to *create* the expectations, and one for the `raw` data, which we will *validate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ref_asset = datasource.add_csv_asset(\"reference_asset\", filepath_or_buffer=\"data/reference/sentiment_reference.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Asset 'reference_asset' already exists.\")\n",
    "    ref_asset = datasource.get_asset(\"reference_asset\")\n",
    "\n",
    "try:\n",
    "    raw_asset = datasource.add_csv_asset(\"raw_asset\", filepath_or_buffer=\"data/raw/feedback.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Asset 'raw_asset' already exists.\")\n",
    "    raw_asset = datasource.get_asset(\"raw_asset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Expectation Suite\n",
    "\n",
    "We will create a new, empty suite called `data_quality_suite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_name = \"data_quality_suite\"\n",
    "try:\n",
    "    context.add_expectation_suite(suite_name)\n",
    "    print(f\"Expectation suite '{suite_name}' created.\")\n",
    "except ge.exceptions.DataContextError:\n",
    "    print(f\"Expectation suite '{suite_name}' already exists.\")\n",
    "\n",
    "# Create a validator using our reference data\n",
    "validator = context.get_validator(\n",
    "    batch_request=ref_asset.build_batch_request(),\n",
    "    expectation_suite_name=suite_name\n",
    ")\n",
    "\n",
    "print(\"Validator created using reference data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Expectations\n",
    "\n",
    "Here we define the \"rules\" for our data based on the `reference` asset. These are the core of our data validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Schema Expectations (Columns)\n",
    "validator.expect_table_columns_to_match_ordered_list([\"id\", \"text\", \"sentiment\"]) # We check the *raw* data schema\n",
    "validator.expect_column_to_exist(\"id\")\n",
    "validator.expect_column_to_exist(\"text\")\n",
    "validator.expect_column_to_exist(\"sentiment\")\n",
    "\n",
    "# 2. ID Column Expectations\n",
    "validator.expect_column_values_to_be_unique(\"id\")\n",
    "validator.expect_column_values_to_not_be_null(\"id\")\n",
    "validator.expect_column_values_to_be_of_type(\"id\", \"int64\")\n",
    "\n",
    "# 3. Text Column Expectations\n",
    "validator.expect_column_values_to_not_be_null(\"text\")\n",
    "validator.expect_column_values_to_be_of_type(\"text\", \"str\")\n",
    "validator.expect_column_value_lengths_to_be_between(\"text\", min_value=5, max_value=500)\n",
    "\n",
    "# 4. Sentiment Column Expectations (Target Variable)\n",
    "validator.expect_column_values_to_not_be_null(\"sentiment\")\n",
    "validator.expect_column_values_to_be_in_set(\"sentiment\", [\"positive\", \"negative\", \"neutral\"])\n",
    "\n",
    "print(\"Expectations added to the validator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Expectation Suite\n",
    "\n",
    "Finally, we save our defined expectations to a JSON file in the `great_expectations/expectations` directory. This suite can now be loaded and run by our Prefect pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "print(f\"Expectation suite '{suite_name}' saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "1.  **Checkpoint:** We will create a Checkpoint (a YAML file) that bundles this suite with a data asset (like our `raw_asset`) to make validation runnable.\n",
    "2.  **Data Docs:** Run `great_expectations docs build` in your terminal to see a beautiful HTML report of these expectations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}