{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Prototyping & MLflow Integration\n",
    "\n",
    "**Objective:** Build a baseline sentiment analysis model, train it on the processed data, and integrate with MLflow for experiment tracking and model registration. The logic developed here will be refactored into our `src/pipeline/tasks` for the production Prefect pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.13.9)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'f:/Projects/ml-retraining-pipeline/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "# Import our config settings\n",
    "# This requires the notebook to be run from the project root\n",
    "# or the 'src' directory to be in the Python path.\n",
    "import sys\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.insert(0, '../src')\n",
    "    if not os.path.exists('../src'):\n",
    "        sys.path.pop(0)\n",
    "        sys.path.insert(0, 'src')\n",
    "\n",
    "# Change CWD to project root if running from /notebooks\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "    print(f\"Changed directory to: {os.getcwd()}\")\n",
    "\n",
    "from src.config.settings import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load & Prepare Data\n",
    "\n",
    "We'll use the `reference` data for this prototype, as it's our clean, validated dataset. In a real pipeline, we'd use the `processed` data, but for this prototype, `reference` is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv(settings.REFERENCE_DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {settings.REFERENCE_DATA_PATH}\")\n",
    "    print(\"Please ensure you have run 'dvc pull' or the file exists.\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = data['text']\n",
    "y = data['sentiment']\n",
    "\n",
    "# Create a train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=settings.MODEL_TEST_SPLIT_SIZE,\n",
    "    random_state=settings.MODEL_RANDOM_STATE,\n",
    "    stratify=y  # Ensure balanced classes in splits\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure MLflow\n",
    "\n",
    "We set the tracking URI (to our local `mlruns` directory) and the experiment name. If the experiment doesn't exist, MLflow creates it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(settings.MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(settings.MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(settings.MLFLOW_EXPERIMENT_NAME)\n",
    "print(f\"MLflow Experiment Name: {experiment.name}\")\n",
    "print(f\"MLflow Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define and Train Model within an MLflow Run\n",
    "\n",
    "We'll use `mlflow.start_run()` to create a new experiment run. Inside this context, we will:\n",
    "1.  Define a `scikit-learn` pipeline (TfidfVectorizer + LogisticRegression).\n",
    "2.  Log hyperparameters (params).\n",
    "3.  Train the model.\n",
    "4.  Evaluate the model and log metrics (accuracy, f1, etc.).\n",
    "5.  Use `mlflow.sklearn.log_model()` to log the trained model pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for our model\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": (1, 2),  # Use unigrams and bigrams\n",
    "    \"tfidf__max_features\": 1000,    # Limit feature space\n",
    "    \"logreg__C\": 1.0,                # Logistic regression regularization strength\n",
    "    \"logreg__solver\": \"liblinear\",\n",
    "    \"logreg__random_state\": settings.MODEL_RANDOM_STATE\n",
    "}\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Starting MLflow Run ID: {run_id}\")\n",
    "    \n",
    "    # 1. Define the model pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('logreg', LogisticRegression())\n",
    "    ])\n",
    "    \n",
    "    # Set parameters\n",
    "    pipeline.set_params(**params)\n",
    "    \n",
    "    # 2. Log parameters\n",
    "    print(\"Logging parameters...\")\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_param(\"test_split_size\", settings.MODEL_TEST_SPLIT_SIZE)\n",
    "    \n",
    "    # 3. Train the model\n",
    "    print(\"Training model...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # 4. Evaluate and log metrics\n",
    "    print(\"Evaluating model and logging metrics...\")\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1_weighted\": f1_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"precision_weighted\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "        \"recall_weighted\": recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    }\n",
    "    \n",
    "    mlflow.log_metrics(metrics)\n",
    "    print(f\"Metrics: {metrics}\")\n",
    "    \n",
    "    # 5. Log the model\n",
    "    print(\"Logging model...\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline,\n",
    "        artifact_path=\"model\",  # This creates a 'model' subdirectory in the artifact store\n",
    "        registered_model_name=settings.MODEL_REGISTRY_NAME # Register the model\n",
    "    )\n",
    "    \n",
    "    # Log a tag for this run\n",
    "    mlflow.set_tag(\"run_type\", \"prototype\")\n",
    "    \n",
    "    print(\"MLflow run complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Review Run in MLflow UI\n",
    "\n",
    "Now, you can check the MLflow UI to see the run, its parameters, metrics, and the registered model.\n",
    "\n",
    "1.  Open a terminal in the project root.\n",
    "2.  Make sure your virtual environment is active.\n",
    "3.  Run `mlflow ui`\n",
    "4.  Open `http://127.0.0.1:5000` in your browser.\n",
    "\n",
    "You should see the `SentimentModelRetraining` experiment with one run. Clicking it will show the logged params and metrics. In the \"Artifacts\" section, you'll see the `model` folder. In the \"Models\" tab, you'll see `prod-sentiment-classifier` with one version (Version 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model from Registry (Simulation)\n",
    "\n",
    "Let's simulate how another service (like `KubeSentiment`) would load this model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model using its registered name and stage\n",
    "# 'None' stage automatically gets the latest version\n",
    "model_uri = f\"models:/{settings.MODEL_REGISTRY_NAME}/None\"\n",
    "\n",
    "print(f\"Loading model from: {model_uri}\")\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "# Test with a sample prediction\n",
    "sample_text = [\"This is a fantastic product!\", \"I am very angry.\"]\n",
    "predictions = loaded_model.predict(sample_text)\n",
    "print(f\"Sample predictions: {list(zip(sample_text, predictions))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
